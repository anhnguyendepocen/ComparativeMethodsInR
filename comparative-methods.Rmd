---
title: "Comparative Methods"
author: "Brian O'Meara"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: bomeara/ComparativeMethodsInR
description: "How to do comparative methods for evolution and ecology"
---

# Prerequisites

This book was created using bookdown. To make it,

`bookdown::render_book("index.Rmd", "bookdown::pdf_book")`


```{r include=FALSE}
devtools::install_github("rstudio/bookdown")
set.seed(1859) # Just to make things repeatable
# automatically create a bib database for R packages
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown', 'ape', 'geiger'
), 'packages.bib')
```

<!--chapter:end:index.Rmd-->

# First steps

First, understand the question you want to answer. There are a wide variety of methods, and they wax and wane in popularity, but the key to doing good science is addressing compelling questions, not using the latest method. Once you have that question, find the appropriate methods (and, depending on how early it is in the study design, the right taxa and data) to address it. Understand how those methods work, including the various ways they can fail (as all can).

Many methods are now implemented in R [@R-base]: the [phylogenetics task view](http://cran.r-project.org/web/views/Phylogenetics.html) has a brief overview. You can also install the relevant packages that are on CRAN and R-Forge using the task view itself:

```
install.packages("ctv")
library(ctv)
install.views("Phylogenetics")
```

Note that this will not install packages that are on github or authors' individual websites. The `devtools` package can be useful for installing packages directly from github.

An important resource for learning about phylogenetics in R is Emmanuel Paradis' book, [Analysis of Phylogenetics and Evolution with R](http://www.springer.com/us/book/9781461417422). This is written by the same person who is the lead author of the essential `ape` package for phylogenetics in R.

<!--chapter:end:00_firststeps.Rmd-->

# Getting data and trees into R
## Data and tree object types
In R, there are many kinds of objects. These kinds are called "classes". For example, take the text string "Darwin".

```{r}
class("Darwin")
```

It is a `character` class. `pi` is a defined constant in R:

```{r}
print(pi)
class(pi)
```

Its class is `numeric` [and note that its value is stored with more precision than is printed on screen].

Objects can sometimes be converted from one class to another, often using an `as.*` function:

```{r}
example.1 <- "6"
print(example.1)
class(example.1)
example.2 <- as.numeric(example.1)
print(example.2)
class(example.2)
example.2 * 7
```

Trying to multiply example.1 by seven results in an error: you are trying to multiply a character string by a number, and R does not automatically convert classes. Classes have many uses in R; for example, one can write a different `plot()` function for each class, so that a tree is plotted one way, while a result from a regression model is plotted a different way, but users just have to call `plot()` on each and R knows what to do.

In phylogenetics, we mostly care about classes for trees, for data, and for things to hold trees and data.

### Tree classes

The main tree class in R is `phylo` and is defined in the `ape` package. Let's look at one in the wild:

```{r}
library(ape)
phy <- rcoal(5) #to make a random five taxon tree
print(phy)
str(phy)
```

This is the one used in most packages. However, it has some technical disadvantages (sensitivity to internal structure, no checking of objects) that has led to the `phylo4` format for trees and `phylo4d` for trees plus data in the `phylobase` package. Other packages add on to the `phylobase` format (i.e., `phytool`'s `simmap` format) but these are typically not shared across packages.

## Sequence data



## Other character data


## Phylogenies
The most common way to load trees is to use `ape`'s functions:

```
phy <- read.tree(file='treefile.phy')
```

To get a tree in [Newick format](http://evolution.genetics.washington.edu/phylip/newicktree.html) (sometimes called Phylip format): essentially a series of parenthetical statements. An example (from `ape`'s documentation) is ```((Strix_aluco:4.2,Asio_otus:4.2):3.1,Athene_noctua:7.3);```. The format name comes from the name of the [lobster house](http://newicks.com) where several major phylogenetic software developers met to agree on a tree format.

You can use the same function to enter tree strings directly, changing the argument from the `file` containing the tree to `text` containing the tree string:

```
phy <- read.tree(text = '((Strix_aluco:4.2,Asio_otus:4.2):3.1,Athene_noctua:7.3);')
```

Note the trailing semicolon.

  One thing that can trip users up with `read.tree()` (and the `read.nexus()` function, below) is that the output class depends on the input. If you read from a file with one tree, the returned output is a single tree object with class `phylo`. You can then use `plot()` on this object to draw the tree, pass this object into a comparative methods package to estimate rates, and so forth. If the file has more than one tree, the returned class is `multiphylo`: `plot()` will automatically cycle through plots as you type return, most comparative method implementations will fail (they are written to expect one tree of class `phylo`, not a vector of trees in a different class). `read.tree()` has an optional `keep.multi` function: if set to TRUE, the class is always `multiphylo`, and you can always get the first tree by getting the first element in the returned object:

```
phy.multi <- read.tree(file='treefile.phy', keep.multi = TRUE)
phy <- phy.multi[[1]]
```

For NEXUS formatted files (Maddison et al., 2007), `ape`'s `read.nexus()` function can pull in the trees (and its `read.nexus.data()` function can pull in data from a NEXUS file). NEXUS is a very flexible format, and there are valid NEXUS files that still cause errors with `ape`'s function. A more robust function to read in NEXUS trees is the package `phylobase`'s `readNexus()` function (note the lack of a period and different capitalization of Nexus from `ape`'s similar function). `phylobase` uses a different structure to store trees than `ape` does.


## Reconciling datasets

<!--chapter:end:20_gettingdata.Rmd-->

# Visualizing data before use
A key step in any analysis is looking at the data. If you have loaded protein coding DNA sequences, are they aligned correctly? Are the codon positions specified correctly? For trait data, is everything measured in the same units, or are some oddly a thousand-fold higher than others? Are you dealing with an older dataset format that uses -1 or 19 for missing data, and have you incorrectly treated those as observations? Is your tree ultrametric?

It is easy to overlook this step, but you can draw the wrong conclusions based on errors at this stage. Most peer reviewers will not notice this, either, so your error could slip into the literature and mislead others. Take the time to get to know your data.

<!--chapter:end:30_visualizingdata.Rmd-->

# Dull model testing

Almost all biologists believe this about the world:

* All species evolve identically
  * Rates of trait evolution are the same
  * Optimal states are the same
  * Speciation rates never change
* Traits are uncorrelated
* Species evolve completely independently
* Extinction never happens
* All evolutionary rates are constant 
    * Across all time
    * Across all space
    
However, a scrappy group of biologists are using comparative methods to attack the mainstream view. For example, using diversification analyses, they can show that extinction can sometimes be greater than zero. Using analyses of trait evolution, they have found that different species actually have different rates of evolution: whale body mass does *not* evolve in the same way bat body mass does. These ideas are rocking the scientific establishment.

Of course, the above is all fiction. We *know* that different things are... different, because they're not the same. We know about extinction, about rates changing over time, about how traits must interact with each other. But the way we do science does not reflect this. Instead, when doing empirical analyses, we focus on rejecting trivial null models, or more simply, dull models. It is useful to show that using a more complex, biologically more credible model is warranted, but too many studies just stop there: a pure birth model is rejected for a logistic growth model for number of species, a single Brownian motion rate model is rejected for an Ornstein-Uhlenbeck model, etc. However, rejecting dull models we did not believe in does not advance science: it tells us more about the power of our study than about actual biological mechanisms. Of course different groups have different rates of evolution: what is the magnitude of the difference? Getting rates with uncertainty is a better way of getting at the biological meaning of differences.

Dull model testing comes up in discussion of a method's fitness, too. The first question asked of a new method, or a published model under attack, is its type I error rate. This is relevant: a method that too often picks an alternate model when the null is true is worrisome. However, it is also not especially relevant biologically. The null model is never true. It may be that due to small sample size, the null is the best-fitting model, but in any empirical scenario the true model is never the null. 

<!--chapter:end:40_dullmodels.Rmd-->

---
title: 'PhyloMeth: Getting Started'
author: "Brian O'Meara"
output: html_document
---

##Objectives
By the end of this week, you will have:

* All required tools installed
* Rudimentary understanding of
    + git
    + unit testing
    + command line
* Ability to do the first assignment
* Ability to talk to classmates
    
Things I want you do are **in bold**.     

In this class we'll be using a variety of programs and technologies. Most are open source: "software that can be freely used, changed, and shared (in modified or unmodified form) by anyone" https://opensource.org. For science, open source software is important: we build on the work of others in making discoveries, and open software allows others to build on it in the same way. It also aids transparency and reproducibility: you can go deep into code to see what is being done, and even if a developer stops supporting software, someone else can take it over and maintain it.

Every student needs a [GitHub](http://github.com) account so we can all use git together. Git is like a mixture of track changes in Microsoft Word, simultaneous editing in a Google Docs form, and a set of files backed up with Dropbox. It is used a lot by people writing software: you can keep track of who has added changes to code, roll back code to an earlier version, and merge code that was worked on different people at the same time. GitHub is a place where a lot of open source software is housed. It offers free accounts (with some restrictions). We'll be using it to do homework and for our discussions.

**Go to [GitHub](http://github.com) now and get an account.**

Once you have an account, **go to https://gitter.im/bomeara/phylometh**. Gitter is a free service that allows for group chats (think of something like Skype chatting). I have made a public group for the course: anyone can join or read the texts. You can download various apps for Gitter for your laptop or phone: https://gitter.im/apps.

You will need a variety of software: **right now, R and git**; eventually, things like MrBayes, Beast, and more. 

On a Mac, it is easiest to first install MacPorts (will help a lot with later steps): see [here](https://www.macports.org/install.php) for how to do that. Then install git. For Linux, you probably already know how to use a package manager. For Windows, there is a git installer, but you might consider something like cygwin as well.

To **install R**, go to your preferred CRAN mirror to download it: I'd suggest https://cran.rstudio.com. What does all this mean? CRAN = Comprehensive R Archive Network: it's where a lot of R packages are stored and readied for installation. A mirror is a computer that stores a copy of files on another computer. This is an old school approach to creating redundancy and convenience: rather than every R user in the world trying to copy a file off of one computer's hard drive connected to the wall with one ethernet cable, there's a whole set of computers, often with one fairly close to the user and so with faster download speeds. However, these can have their own issues: some might not update their copies often (look at [this](https://cran.r-project.org/mirmon_report.html) to see some of these), some are slow, etc. A more modern approach is a content delivery network (CDN): a distributed set of servers, and a user is automatically routed to the best one for her or him given location, speed, server status, etc. When you watch a movie on Netflix, you're not using the same server as everyone else, for example. The R mirror I suggest you use is one run by the RStudio commercial company: it's actually a CDN and is secure and fast. Unlike the other R mirrors, it also records number of times each package is downloaded, which can be useful data for package authors.

Once R is installed, it's time to install packages. Most packages are on CRAN, though an increasing number are just on R-Forge, GitHub, or other sites. (CRAN has volunteers who check packages thoroughly to make sure they install on a variety of operating systems, keep to modern coding standards, deal with licensing appropriately, etc. (they can't, of course, ensure that the statistical approaches are sound): this is great for users but some developers find it difficult, so put their packages elsewhere).

One helpful way to navigate the array of R packages is through [task views](https://cran.r-project.org/web/views/): curated lists of packages. You can also **install all the packages in a view** and those they depend on. Do this:

```
install.packages("ctv") #the CRAN task view package
```

And then choose the packages to install. For this class, I'd suggest Phylogenetics and WebTechnologies:

```
library(ctv) #to load the package
install.views(c("Phylogenetics", "WebTechnologies"))
```

We will be using unit testing to check homework. For this, you will need to install the `testthat` package.

```
install.packages("testthat")
```

Bioconductor is a separate repository with lots of packages for dealing with genetic data, especially nextgen data. To install this and a few initial packages:

```
source("http://bioconductor.org/biocLite.R")
biocLite()
biocLite("Biostrings")
```

Once you have all this done, it is time to start the exercise:
https://classroom.github.com/assignment-invitations/c28cc1d5a6c9e8151508df973ab23e70


<!--chapter:end:50_GettingStarted.Rmd-->

---
title: 'PhyloMeth: Continuous Traits'
author: "Brian O'Meara"
output: html_document
date: '`r format(Sys.time(), "Last updated %H:%M:%S %p %d %B, %Y")`'
---



##Objectives
By the end of this week, you will:

* Understand various continuous trait models
* Be able to run key software

Make sure to **read the relevant papers**: https://www.mendeley.com/groups/8111971/phylometh/papers/added/0/tag/week7/    

Last week we did some simulation under Brownian motion and talked about using this model for dealing with correlations (as in independent contrasts). The central limit theorem is great: as you add changes, you converge back to a normal distribution. But what if the changes aren't i.i.d.? For example, what if the rate of body size evolution of birds dramatically increased once other dinosaurs went extinct? We would have variance accumulating linearly with time before and after the KT extinction, but the rate of increase would be different between the two time periods. 

**Do the homework at** https://github.com/PhyloMeth/ContinuousTraits

You will:
* Use Geiger to estimate rate of evolution under Brownian motion
* Figure out what the units are
* Try other ways of scaling rates
* Compare different models using OUwie
* Do model comparison

<!--chapter:end:60_Continuous.Rmd-->

---
title: 'PhyloMeth: Brownian Motion and Correlations'
author: "Brian O'Meara"
output: html_document
date: '`r format(Sys.time(), "Last updated %H:%M:%S %p %d %B, %Y")`'
---

*In progress*

##Objectives
By the end of this week, you will:

* Understand the importance of dealing with correlations in an evolutionary manner
* Know methods for looking at correlations of continuous and discrete traits
* Be able to point to reasons to be concerned.

Make sure to **read the relevant papers**: https://www.mendeley.com/groups/8111971/phylometh/papers/added/0/tag/week6/    
## Brownian motion

First, let's get a tree:

```{r, warning=FALSE, message=FALSE, fig.height=12}
library(rotl)
library(ape)
phy <- get_study_tree("ot_485", "tree1")
plot(phy, cex=0.5)
axisPhylo(backward=TRUE)
```

Note that this tree is a chronogram. 

Let's simulate data on this tree. But what model to use? For now, let's assume we are looking at continuous traits, things like body size. Over evolutionary time, these probably undergo a series of changes that then get added up. A species has an average mass of 15 kg, then it goes to 15.1 kg, then 14.8 kg, and so forth. But how could those changes be distributed?

Start with a uniform distribution. Take a starting value of 0, then pick a number from -1 to 1 to add to it (in other words, `runif(n=1, min=-1, max=1)`). There are efficient ways to do this for many generations, but let's do the obvious way: a simple `for` loop. Do it for 100 generations.

```{r}
ngen <- 100
positions <- c(0, rep(NA,ngen))
for (i in sequence(ngen)) {
  positions[i+1] <- positions[i] + runif(1,-1,1)
}
plot(x=positions, y=sequence(length(positions)), xlab="trait value", ylab="generation", bty="n", type="l")
```

We can repeat this simulation many times and see what the pattern looks like:

```{r}
ngen <- 100
nsims <- 500
final.positions <- rep(NA, nsims)
# make a plot to hold our lines
plot(x=c(-1,1)*ngen, y=c(1, 1+ngen), xlab="trait value", ylab="generation", bty="n", type="n") 
for (sim.index in sequence(nsims)) {
  positions <- c(0, rep(NA,ngen))
  for (i in sequence(ngen)) {
    positions[i+1] <- positions[i] + runif(1,-1,1)
  }
  lines(positions, sequence(length(positions)), col=rgb(0,0,0,0.1))
  final.positions[sim.index] <- positions[length(positions)]
}
```

Well, that may seem odd: we're adding a bunch of uniform random values between -1 and 1 (so, a flat distribution) and we get something that definitely has more lines ending up in the middle than further out. Look just at the distribution of final points:

```{r}
plot(density(final.positions), col="black", bty="n")
```

Which looks almost normal. Ok, let's try a weird distribution:

```{r}
rweird <- function() {
  displacement <- 0
  if(runif(1,-2,2) < .1) {
    displacement <- rnorm(1, 7, 3) + runif(1,0,7)
  } else {
    displacement <- 0.5 * rexp(1, 0.3) - 1 
  }
  displacement <- displacement + round(runif(1,1,100) %% 7)
  return(displacement)  
}

plot(density(replicate(100000, rweird())), bty="n")
```

When we ask `rweird()` for a number it sometimes gives us a normally distributed number multiplied by a unifor distribution, other times it gives us an exponentially distributed number, and then adds the remainder that comes when you divide a random number by 7. So, not exactly a simple distribution like uniform, normal, or Poisson. So, repeating the simulation above but using this funky distribution:

```{r}
ngen <- 100
nsims <- 500
final.positions <- rep(NA, nsims)
# make a plot to hold our lines
plot(x=c(-100,1200), y=c(1, 1+ngen), xlab="trait value", ylab="generation", bty="n", type="n") 
for (sim.index in sequence(nsims)) {
  positions <- c(0, rep(NA,ngen))
  for (i in sequence(ngen)) {
    positions[i+1] <- positions[i] + rweird()
  }
  lines(positions, sequence(length(positions)), col=rgb(0,0,0,0.1))
  final.positions[sim.index] <- positions[length(positions)]
}
```

And now let's look at final positions again:

```{r}
plot(density(final.positions), col="black", bty="n")
```

Again, it looks pretty much like a normal distribution. You can try with your own wacky distribution, and this will almost always happen (as long as the distribution has finite variance). 

Why?

Well, think back to stats: why do we use the normal distribution for so much?

Answer: the central limit theorem. The sum (or, equivalently, average) of a set of numbers pulled from distributions that each have a finite mean and finite variance will approximate a normal distribution. The numbers could all be independent and come from the same probability distribution (i.e., could take numbers from the same Poisson distribution), but this isn't required.

Biologically, the technical term for this is awesome. We know something like a species mean changes for many reasons: chasing an adaptive peak here, drifting there, mutation driving a it this way or that, etc. If there are enough shifts, where a species goes after many generations is normally distributed. For two species, there's one normal distribution for their evolution from the origin of life (or the start of the tree we're looking at) to their branching point (so they have identical history up to then) then each evolves from that point independently (though of course in reality they may interact; the method that's part of the grant supporting this course allows for this). So they have covariance due to the shared history, then accumulate variance independently after the split. We thus use a multivariate normal for multiple species on the tree (for continuous traits), but it again is due to Brownian motion. **This mixture of independent and shared evolution is quite important: it explains why species cannot be treated as independent data points, necessitating the correlation methods that use a phylogeny in this week's lessons.**

However, in biological data there are (at least) two issues. One is that in some ways a normal distribution is weird: it says that for the trait of interest, there's a positive probability for any value from negative infinity to positive infinity. "Endless forms most beautiful and most wonderful have been, and are being, evolved" [Darwin] but nothing is so wonderful as to have a mass of -15 kg (or, for that matter, 1e7 kg). Under Brownian motion, we expect a displacement of 5 g to have equal chance no matter what the starting mass, but in reality a shrew species that has an average mass of 6 g is less likely to lose 5 g over one million years than a whale species that has an average adult mass of 100,000,000 g. Both difficulties go away if we think of the displacements not coming as an addition or subtraction to a species' state but rather a multiplying of a state: the chance of a whale or a shrew increasing in mass by 1% per million years may be the same, even if their starting mass magnitudes are very different. Conveniently, this also prevents us getting zero or lower for a mass (or other trait being examined). This works if we use the **log of the species trait and treat that as evolving under Brownian motion**, and this is why traits are commonly transformed in this way in phylogenetics (as well they should be). 

The other issue is that the normal approximation might not hold. For example, if species are being pulled back towards some fixed value, the net displacement is not a simple sum of the displacements: we keep getting pulled back, in effect eroding the influence of movements the deeper they are in the past: thus the utility of Ornstein-Uhlenbeck models. There may also be a set of displacements that all come from one model, then a later set of displacements that all come from some different model: we could better model evolution, especially correlation between species, by using these two (or more) models rather than assume the same normal distribution throughout time: thus the utility of approaches that allow different parameters or even different models on different parts of the tree.

## Discrete models

Much of the above discussion (except for the part about covariance due to shared history) focuses on continuous traits. However, some traits are better thought of as discrete (btw, note the spelling here: having one, two or eight eyes is a *discrete* trait: individually separate and distinct. Forming an enclosed bower for hidden mating is a *discreet* trait. The former is generally far more biologically relevant). This is always an approximation. Think of something like limbs: they seem distinct enough that we even name some groups by their count: tetrapods, hexapods. Except that when we look closely enough, it becomes fuzzy: insect mouthparts are derived from limbs, for example, so should we count these highly modified limbs as limbs (and if not, where in evolution have they become sufficiently modified to no longer count? And are nymphalid butterflies tetrapods under that definition yet?). Are modern whales thought to have four limbs, even though two are extremely vestigial? Often for neontologists problematic organisms with intermediate counts are conveniently extinct (so long, *Basilosaurus*), so we can ignore this fuzziness, but it is often there (and paleontologists are confronted with it more often). 

But, let's assume we can discretize traits and carry on. The simplest discretization is binary: 0 or 1, often absence or presence (but could be yellow or blue, etc.). Most models are like our commonly used DNA models: continuous time with discrete changes, using the same rates throughout. It is like a model for when an autonomous car will have an accident: assuming the car works perfectly (gives a whole new meaning to "blue screen of death") there's still a chance that at some point a human is going to run into it. There's a per hour chance of an accident: let's assume in each hour there's a 0.03% chance of our autonomous car having an accident (very roughly based on Google's experience, assuming a 40 MPH average speed). So the probability of having no accident in the first hour of driving is 99.97%; the probability of having no accidents in the first 40 hours of driving is 99.97% ^ 40 = 98.8%. The number of accidents is Poisson-distributed; the wait time between accidents is exponentially distributed. This is the model commonly used in phylogenetics for discrete traits, though sometimes with more complexity: one could move (with some rate) between two different rates, as in a covarion model, for example. A very different model is Felsenstein's threshold model, which we will discuss in a few weeks. For now, though, just envision models with a fixed rate of change between states as long as other characters don't change; it's possible, though, that the state of other characters do affect these rates (which is what correlation tests investigate). For example, the probability of switching from clawed feet to flippers for forelimbs is probably much higher for species that live in water than on land.

## Correlation
For this week, bring your data and a tree for those taxa. Fork https://github.com/PhyloMeth/Correlation and then add scripts there. When you're done, do a pull request. Note if you add data to that directory and commit it, it'll be uploaded to public GitHub. Probably not a big deal, unless you want to keep your data secret and safe (Lord of The Rings reference; c.f. `phangorn` package).

**Do independent contrasts** using `pic()` in `ape`. Remember to 1) positivize the contrasts (this is *not* the same as doing `abs()`). From the Garland et al. paper, think about ways to see if there are any problems. How do contrasts affect the correlations?

**Do Pagel94**
There are at least three ways to do this in R: in the `phytools`, `diversitree`, and `corHMM` packages. With `phytools`, it's pretty simple: use the `fitPagel()` function. With the others, you have to specify the constraint matrices (this allows you to do Pagel-style tests but on a wider range of models). Think about what you should assume at the root state: canonical Pagel94 assumes equal probabilities of each state at the root, but that might be a bad assumption for your taxa.

**Use another correlation method**
Perhaps phyloGLM? Use the `phylolm` package, or some other approach to look at correlations.

<!--chapter:end:70_Correlations.Rmd-->

---
title: 'PhyloMeth: Discrete Traits'
author: "Brian O'Meara"
output: html_document
date: '`r format(Sys.time(), "Last updated %H:%M:%S %p %d %B, %Y")`'
---



##Objectives
By the end of this week, you will:

* Understand how to incorporate rate heterogeneity in discrete trait models
* Be able to explain how to test hypotheses about univariate trait evolution.

Make sure to **read the relevant papers**: https://www.mendeley.com/groups/8111971/phylometh/papers/added/0/tag/discrete/  

And **do the relevant exercise**:
https://github.com/PhyloMeth/DiscreteTraits

<!--chapter:end:80_Discrete.Rmd-->

---
title: 'PhyloMeth: Diversification'
author: "Brian O'Meara"
output: html_document
date: '`r format(Sys.time(), "Last updated %H:%M:%S %p %d %B, %Y")`'
---



##Objectives
By the end of this week, you will:

* Understand diversification models that don't incorporate traits
* Be able to estimate diversification parameters for your data

Make sure to **read the relevant papers**: https://www.mendeley.com/groups/8111971/phylometh/papers/added/0/tag/diversification/  

And **do the relevant exercise**:
https://github.com/PhyloMeth/Diversification

<!--chapter:end:90_Diversification.Rmd-->

---
title: 'PhyloMeth: SSE Methods'
author: "Brian O'Meara"
output: html_document
date: '`r format(Sys.time(), "Last updated %H:%M:%S %p %d %B, %Y")`'
---



##Objectives
By the end of this week, you will:

* Understand models that look at the effects of traits on diversification
* Understand some of the problems with these
* Be able to categorize Type I and Type II errors and talk about their relevance.

Make sure to **read the relevant papers**: https://www.mendeley.com/groups/8111971/phylometh/papers/added/0/tag/sse/    

At this point in the course, you should be familiar with working through getting software running. If you want more practice, you could use the [vignette](https://cran.r-project.org/web/packages/hisse/vignettes/hisse-vignette.html) written by Jeremy Beaulieu for hisse; running diversitree is similar. However, I think a bigger thing to discuss is how we understand a method is working well, and how we assess whether results are believable. This is especially prominent in discussions of diversification. One great example of this discussion in the literature:

* [Nee et al. (1994)](http://rstb.royalsocietypublishing.org/content/344/1307/77) "Extinction rates can be estimated from molecular phylogenies"
* [Rabosky (2009)](http://onlinelibrary.wiley.com/doi/10.1111/j.1558-5646.2009.00926.x/abstract) "Extinction rates should not be estimated from molecular phylogenies" 
* [Beaulieu and O'Meara (2015)](http://onlinelibrary.wiley.com/doi/10.1111/evo.12614/abstract) "Extinction rates can be estimated from moderately sized molecular phylogenies"
* [Rabosky (2015)](http://onlinelibrary.wiley.com/doi/10.1111/evo.12820/full) "Challenges in the estimation of extinction from molecular phylogenies: A response to Beaulieu and O'Meara"

And that probably isn't the last word on this (no, I'm not planning something at the moment). However, despite all the hemming and hawing over what people *should* do, people still keep using methods: folks using `diversitree`, `BAMM`, `geiger`, or many other approaches merrily estimate extinction rates as a necessary part of their analyses, though they usually don't focus on them in their studies, instead focusing on diversification (speciation - extinction) or speciation alone. (Note: it is possible to do diversification approaches without estimating extinction rates: one could fix extinction rate at a known value (perhaps using the average duration of fossil "species" to get a fixed estimate), though what is usually done is to fix extinction at 0 (this is called a Yule model, if speciation is constant). This is a bit weird when you think about it: one of the few things we truly know in biology without a doubt is that extinction is far, far from zero in general (though this wasn't [really discovered in Western science until the 19th century](http://www.ucmp.berkeley.edu/mammal/artio/irishelk.html))). But as skeptical scientists, we should aim higher. It's often tempting, especially as students or postdocs facing a difficult job market, to focus on what we can get out: is there an exciting result that can get past peer review and build our fame (maybe, in our tiny circle) and fortune (ha!). But it's important to take a step back: are we confident enough, truly (not just based on the p-value) that our results are actually discoveries about nature? Diversification is an area (ancestral state estimation is another) where the reality of results is especially worrisome. Take, for example, [Etienne et al. (2016)](http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12565/abstract), who as part of a broader simulation study, analyzed a dataset of 25 *Dendroica* warbler species, which is a classic dataset for these studies, fitting a logistic growth model. They found that depending on how the model was conditioned[^1], something most users would ignore, the estimated carrying capacity (in this model, the number of warbler species at which speciation equals extinction; in other models, this would be the number of species at which speciation is zero, with extinction *always* set to zero) for warblers is 24.59 or 6.09 or 0.656 species. That means, depending on how one conditions, we should expect the number of *Dendroica* warblers to stay exactly where it is, crash to around six species, or crash to half a species [and of course, in the latter two cases, stochastic change in number of species will lead to them hitting zero species fairly quickly, which is an absorbing state]. Given the sensitivity of the model to factors like conditioning, any result has to be taken with a great deal of skepticism.

For trait based models, the same history of finding problems and the solutions (or partial solutions) has arisen (there is a paper coming out in the *American Journal of Botany* in May 2016 that goes into this in more detail). The most relevant parts:

[Maddison (2006)](http://onlinelibrary.wiley.com/doi/10.1111/j.0014-3820.2006.tb00517.x/abstract) (and there were similar points made by others earlier) showed that transition rates and diversification rates can be hard to distinguish. Oversimplifying a bit, but if the rate of going from state 0 to state 1 is higher than the reverse rate, then over time there should be many more taxa in state 1 than 0. If the diversification rate in state 1 is higher than in state 0, there will tend to be more taxa in state 1 than 0. So if you see a tree with more state 1 than 0, is it higher transition rate to 1, or higher diversification in 1? Or is it lower transition to 1 but a high enough diversification rate that it overwhelms it? Or just chance? The paper identified the problem but not the solution: the last line of the abstract is "Studies of biased diversification and biased character change need to be unified by joint models and estimation methods, although how successfully the two processes can be teased apart remains to be seen."

This was followed up by [Maddison et al. (2007)](http://sysbio.oxfordjournals.org/content/56/5/701.short) who figured out a method to deal with this: BiSSE. And systematists looked at it, and it was good. There is now a bestiary of similar *SSE models for different kinds of data: QuaSSE, GeoSSE, MuSSE, ClaSSE, and more. 

However, there are concerns. The original paper showed that estimating extinction was hard to do accurately, and other papers (Davis et al, 2013) showed that you needed a hundreds of species to find significant results (sorry, *Dendroica* -- you will forever remain a mystery). However, a particular scary paper (and talk at Evolution, where it was presented) was [Maddison & FitzJohn (2014)](https://sysbio.oxfordjournals.org/content/early/2014/10/23/sysbio.syu070.full). Their paper mostly discussed correlated characters, but has relevance to SSE approaches. If you have a single change on a tree, you don't know if a higher rate in the clade descended from that branch is due to that trait, some other trait changing on that branch, or some other trait changing on the tree but in a way that makes it mostly on one part of the tree. [Rabosky & Goldberg (2015)](http://sysbio.oxfordjournals.org/content/early/2015/01/18/sysbio.syu131) showed that the way many people interpret BiSSE results, as a testing of a null hypothesis, can be misled if part of the null hypothesis is wrong but not the part you're interested in. [Beaulieu & O'Meara (2016)](http://sysbio.oxfordjournals.org/content/early/2016/03/25/sysbio.syw022.abstract) address some of the Maddison & FitzJohn issues (a character changing elsewhere on the tree driving a result) but not all the issues (a single change being sufficient to give substantial support for an idea that that trait is driving diversification). 

### Discussion prompts

1. What is Type I and Type II error?
2. Do we care about them? Why or why not?
3. Compare and contrast
    + model selection
    + null hypothesis rejection
    + multimodel inference
    + parameter estimation
4. What is a good null model for trait diversification?
5. What is the model we use?
6. How do you know a method is good enough to use?
    + in general
    + for *your* data
7. Given the controversies about diversification methods, are you willing to use them? Defend your view!

For a lot of these questions, there isn't a right answer (at least, not one I know, and certainly not an agreement in the field). But it's worth thinking about as you develop your research career. 


[^1]: Conditional probability is the probability of an event given that some other event has occurred. For example, you could use past information from your department to estimate `Prob(getting tenure)`, but it is different if you use information that another event has occurred: `Prob(getting tenure | made major discovery in evolution)` is different from `Prob(getting tenure | four years since last publication)`. In this domain (diversification alone and diversification plus trait models), we condition on actually having a tree to look at: if the true model is speciation rate equals extinction rate, there's a good chance that most clades starting X million years ago will have gone extinct, so the ones we see diversified unusually quickly, and this has to be taken into account. The example I usually use for this is the idea that dolphins rescue drowning sailors. It's known dolphins push interesting objects in the ocean. We could interview previously drowning sailors that dolphins pushed towards shore, and they'll all say that dolphins saved them, but it's very hard to interview sailors the dolphins pushed the other way. As always, Randall Munroe's XKCD explains conditional probability best:  
![Seashell XKCD](https://imgs.xkcd.com/comics/seashell.png)


<!--chapter:end:91_SSE.Rmd-->

---
title: 'PhyloMeth: RAxML'
author: "Brian O'Meara"
output: html_document
---

##Objectives
By the end of this week, you will:

* Have RAxML installed
* Be able to do an analysis with likelihood with various models
* Understand partitioning
* Be able to use a variety of character types
    
RAxML (Stamatakis, 2014) is a very popular program for inferring phylogenies using likelihood, though there are many others. It is notable for being able to infer trees for tens of thousands of species or more. New versions can use DNA, amino acid, SNP, and/or morphological characters.

## Install RAxML

To begin, **install RAxML**. Follow the instructions in Step 1 of http://sco.h-its.org/exelixis/web/software/raxml/hands_on.html. For the fewest issues, just do `make -f Makefile.gcc` on the command line (not in R) to compile the basic vanilla version. For actual work, you'll likely find the versions with SSE3 and/or PTHREADS will work faster. On a Mac (Linux is similar; RAxML has [binaries](https://github.com/stamatak/standard-RAxML/tree/master/WindowsExecutables_v8.2.4)), the easiest way to get use this would be:

```
git clone git@github.com:stamatak/standard-RAxML.git
cd standard-RAxML
make -f Makefile.gcc 
```

If compiling went correctly, you should see a line like
```
gcc  -o raxmlHPC axml.o  optimizeModel.o multiple.o searchAlgo.o topologies.o parsePartitions.o treeIO.o models.o bipartitionList.o rapidBootstrap.o evaluatePartialGenericSpecial.o evaluateGenericSpecial.o newviewGenericSpecial.o makenewzGenericSpecial.o   classify.o fastDNAparsimony.o fastSearch.o leaveDropping.o rmqs.o rogueEPA.o ancestralStates.o  mem_alloc.o  eigen.o -lm
```

**Now you need to put the program in a path.** This is where your computer looks for programs to run. If you type a program name, like `ls` or `raxmlHPC`, your computer checks the folders indicated in the path for a program of this name; when it finds one, it runs that. You can see your path by typing `echo $PATH`. If you want to run a program, like the newly compiled `raxmlHPC`, you have two options: you can specify where it is each time you want to run it, or you can put it in a folder in your existing path. The former becomes a pain, so I'd recommend the latter. `/usr/bin` is in your path, but this is reserved for programs your computer needs to run -- don't mess with it. I'd suggest putting it in `/usr/local/bin`. To do this, type

```
sudo cp raxmlHPC /usr/local/bin/raxmlHPC
```

`sudo` means superuser do. It's a very powerful command. Generally, typing on the command line you can delete files that are important to you, but it's hard to utterly destroy your computer; with superuser abilities, you could delete key files.

![Sudo sandwich from xkcd](http://imgs.xkcd.com/comics/sandwich.png)

Ok, so we now have RAxML installed. To run it, you could use the very handy `ips` package to call it from R, but it doesn't have an interface to all of the relevant commands. Instead, we're going to just create some commands to run ourselves.

First, we need sample data sets. We will be using ones, modified somewhat, from [this tutorial](http://sco.h-its.org/exelixis/web/software/raxml/hands_on.html). The original files are [here](http://sco.h-its.org/exelixis/resource/download/hands-on/Hands-On.tar.bz2) but the modified ones are in the [repository for this PhyloMeth exercise](https://github.com/PhyloMeth/LikelihoodTrees) in the `/inst/extdata` folder.

Until now, we've seen NEXUS files, which can include data blocks. RAxML uses Phylip-formatted files instead, which are simpler: a line that has the number of taxa and the number of sites, followed by one line per taxon with the taxon name, a space, and then the characters (though there could be interleaving). 

## Morphology search

First, we are going to examine morphology using likelihood. While morphology is typically analyzed with parsimony, there are models for morphology (i.e., Lewis 2001) and research suggests (Wright & Hillis, 2014) that such models outperform parsimony for morphology, in addition to being less prone (in theory) to long branch attraction (Felsenstein 1978). Therefore, absent strongly held concerns rooted in an [epistemological paradigm](http://onlinelibrary.wiley.com/doi/10.1111/cla.12148/full), it seems prudent to use a parametric model for morphology (note this can be done in likelihood or Bayesian contexts).

Get the [exercise](https://github.com/PhyloMeth/LikelihoodTrees) and **complete the `InferMorphologyTree_exercise` function in `exercise.R`**. Also, look at the data in a text editor to get a sense of the structure. Which taxa are going to be lumped into clades, do you think? Some important things to note:

*  Morphology (as well as some other data, such as SNPs) often includes only variable sites. This can cause a problem if not accounted for (it looks like all sites are evolving really fast, because the slow ones are ignored). There are corrections for this, three of which are implemented in RAxML. 
*  RAxML creates a starting tree, then does a parsimony optimization, then likelihood. This is not a full parsimony search, though.
*  Remember that for nearly all tree searches, heuristic methods are used. That means that you are not guaranteed to get the best tree; given the size of tree space, one could almost say you're guaranteed not to find the best tree.
*  Computers are great at being logical. The downside is that they are terrible at being random. They often use the current time as a "seed" to get a pseudorandom number. You could think of it (this is more of an analogy than a description) as if the computer had a long table of stored "random" numbers, and that it started using numbers at the row corresponding to the number of seconds elapsed between the current time and some fixed date in the past. If you start two runs at different times, they'll have different numbers, but if you start them at the same time, they'll have the same ones. For tree search, there are often random moves: which branch is broken off and moved somewhere else. If you start two searches at the same time, thinking you're doing two independent searches, they'll perform exactly the same, despite the "randomness". RAxML asks users to supply a random number seed to it. If you use the same one across runs, they'll be exactly the same.

## DNA

Most phylogenetic analyses for extant organisms use sequence data. This is often presented as DNA, though sometimes the data are translated to amino acids instead. Usually sequences from multiple genes are concatenated. There are a wide range of models available for sequence evolution. For DNA, the most popular remains general time reversible (GTR): a model that allows for a different transition rate between every pair of nucleotides, subject to the constraint that the rate from nucleotide *i* to *j* is the same as the rate from *j* to *i*. Different sites evolve at different rates (think of the sites coding for the active site of an enzyme versus those in an intron that has little to no functional purpose). One way to model this heterogeneity is with a gamma distribution: the likelihood is evaluated using several different rates for that site (Yang 1995). One can also apply partitions: allow different sections of the data to have different rates. This is commonly done to allow first, second, and third codon positions to have different rates, or to allow different genes to have different models of evolution. This can offer dramatic improvements in the fit of a model to the data; it is especially important when dealing with gappy data, such as cases where one gene is present for all taxa but another gene has ben sampled for only a subset of taxa.

**Do InferDNATreeWithBootstrappingAndPartitions_exercise() in the homework**. Once this is done, install this homework library into R. From the folder containing the homework:

```
R CMD INSTALL LikelihoodTrees
```

Then, in R: 

```
library(PhyloMethLikelihoodTrees)
results <- InferDNATreeWithBootstrappingAndPartitions_exercise()
```

Though you may have to include other arguments (especially `input.path`). 

You can **plot your final tree**: 

```
library(ape)
plot.phylo(results$ml.with.bs.tree, show.node.label=TRUE)
```

This shows the branch lengths of the best ML tree and the bootstrap proportions. This is from a non-parametric bootstrap (Felsenstein 1985): the columns of data are sampled with replacement and then a tree search is redone. The more times a bipartition (an edge) on a tree is recovered, the more confidence we have in it (but this is not the same as the probability of it being true). Note one common error: the numbers reported, and in this case shown at nodes, are *not* properties of a node or a clade: they are bipartitions: taxa A, C, E fall attach (perhaps through other nodes) to one end of an edge, and taxa B, D, E, F, G, H are attached to the other end. 

<!--chapter:end:94_RAxML.Rmd-->

---
title: 'PhyloMeth: Gene Tree Species Tree'
author: "Brian O'Meara"
output: html_document
---

##Objectives
By the end of this week, you will:

* Understand why gene trees and species trees may not always agree
* Know about some of the approaches in this area
* Understand about phylogenetic networks

Make sure to read the relevant papers: https://www.mendeley.com/groups/8111971/phylometh/papers/added/0/tag/week4/
    
First, we will be looking at distinctions between gene trees and species trees. For this, we'll be using Liang Liu's phybase package. This isn't on CRAN, but it is available on his [website](https://faculty.franklin.uga.edu/lliu/content/phybase?). However, we'll be using a version I modified slightly and put on github. Liang's package uses [newick](http://evolution.genetics.washington.edu/phylip/newicktree.html) format (yes, named after the [restaurant](http://newicks.com)), but with additional options for including population size (branch width, included as a `#` sign followed by a number) as well as the more traditional branch length. Both matter for coalescence: two copies are much more likely to have coalesced on a long, narrow branch than a short, fat one. Most packages in R instead use ape's `phylo` format, so I wrote a few functions to deal with that. This version of the package is on github; to install it:

```
library(devtools)
install_github("bomeara/phybase")
```

For real science, Liang's is the canonical one (and definitely cite his) but this will be easier for us to use for class. 

First, get a tree from Open Tree of Life. We'll get a recent plant tree from Beaulieu et al:

```{r, warning=FALSE, message=FALSE}
library(rotl)
library(ape)
phy <- get_study_tree("ot_485", "tree1")
plot(phy, cex=0.3)
```

Let's simplify by dropping some taxa:

```{r, warning=FALSE, message=FALSE}
library(geiger)
phy <- drop.random(phy, Ntip(phy) - 10)
plot(phy)
axisPhylo()
```


We can simulate gene trees on this tree:
```{r, warning=FALSE, message=FALSE}
library(phybase)
gene.tree <- sim.coaltree.phylo(phy, pop.size=1e-12)
plot(gene.tree)
```



And it probably looks very similar to the initial tree:
```{r, warning=FALSE, message=FALSE}
library(phytools)
plot(cophylo(phy, gene.tree, cbind(sort(phy$tip.label), sort(gene.tree$tip.label))))
```

[Note I'm being a bit sloppy here: the initial branch lengths of the tree we used are in millions of years (i.e., 5 = 5 MY) while the coalescent sim is treating these as coalescent time units: there would be even lower chance of incongruence if we converted the former into the latter. Unfortunately, the simulator fails with branch lengths that are realistically long for this tree.]

So, does this mean gene tree species tree issues aren't a problem?

Well, it depends on the details of the tree. One common misconception is that gene tree species tree issues only relate to trees for recent events. This problem can happen any time there are short, fat branches, where lack of coalescence of copies can occur. 

```{r, warning=FALSE, message=FALSE}
species.tree <- rcoal(7)
species.tree$edge.length <- species.tree$edge.length / (10*max(branching.times(species.tree)))
gene.tree <- sim.coaltree.phylo(species.tree)
plot(cophylo(species.tree, gene.tree, cbind(sort(species.tree$tip.label), sort(gene.tree$tip.label))))
```

You should see (in most iterations), the above code giving a mismatch between the gene tree and the species tree (the species tree has little height). Now, let's lengthen the tips of the species tree:

```{r, warning=FALSE, message=FALSE}
tip.rows <- which(species.tree$edge[,2]<=Ntip(species.tree))
species.tree2 <- species.tree
species.tree2$edge.length[tip.rows] <- 100 + species.tree2$edge.length[tip.rows]
gene.tree2 <- sim.coaltree.phylo(species.tree2)
plot(cophylo(species.tree2, gene.tree2, cbind(sort(species.tree2$tip.label), sort(gene.tree2$tip.label))))
```

It looks like a mismatch, but it's hard to see, since the tips are so long. So plot the cladogram instead [we need to manually change branch lengths to do this, though note we do not resimulate the gene tree].

```{r, warning=FALSE, message=FALSE}
species.tree2.clado <- compute.brlen(species.tree2)
gene.tree2.clado <- compute.brlen(gene.tree2)
plot(cophylo(species.tree2.clado, gene.tree2.clado, cbind(sort(species.tree2.clado$tip.label), sort(gene.tree2.clado$tip.label))))
```

So we can see that even though the relevant divergences happened long ago, gene tree species tree issues are still a problem. 

<!--chapter:end:95_GeneTreeSpeciesTree.Rmd-->

---
title: 'PhyloMeth: Dating'
author: "Brian O'Meara"
output: html_document
---

##Objectives
By the end of this week, you will:

* Understand dating algorithms
* Be able to use r8s and BEAST
* Be afraid of calibrations

Make sure to **read the relevant papers**: https://www.mendeley.com/groups/8111971/phylometh/papers/added/0/tag/week5/    

To do this week's assignments, you will have to:

* **Download and install r8s** from http://loco.biosci.arizona.edu/r8s/ 
* **Download BEAST2 and Beauti** (come together), **TreeAnnotator**, and **Tracer** from http://beast2.org and http://beast.bio.ed.ac.uk/tracer.
* **Install a tweaked version of Geiger**
    - `library(devtools)`
    - `install_github("bomeara/geiger-v2")` (eventually I'll make a pull request)
 
## BEAST 
For the BEAST part, we're not going to do our testing via R package approach: dealing with file paths and such are too problematic. So we'll just go through an exercise, but unlike many canned tutorials, you'll have to figure out what to do at stages. Note that this is based on the [tutorial](https://github.com/CompEvol/beast2/blob/master/doc/tutorials/DivergenceDating/DivergenceDatingTutorialv2.0.3.pdf?raw=true) by Drummond, Rambaut, and Bouckaert. A tutorial that provides far more background info and other context than anything on the Beast2 website is Tracy Heath's [tutorial](http://phyloworks.org/workshops/DivTime_BEAST2_tutorial_FBD.pdf) from the Bodega Bay Workshop in Applied Phylogenetics: if you are going to run BEAST, read that. There is also now a [book](http://beast2.org/book/) that you can buy.

BEAST uses XML files for commands, rather than NEXUS files (though it does use NEXUS files for data). This XML format is different from [NeXML](http://www.nexml.org) and [PhyloXML](http://www.phyloxml.org), two other XML formats proposed for phylogenetics (though, as of now, they're still not used much in the field -- most students won't encounter them). These files are often made using the program BEAUTi (also from the BEAST developers) and then, sometimes, hand edited. 

1. **Import primates-mtDNA.nex** into BEAUTi; it's includided in examples/nexus in the BEAST folder. Note it's File -> Import alignment.
2. But wait -- what are you loading? Have you looked at the file? Open it in a text editor (or R, or Mesquite) and look at it. Do you believe the sequences are aligned properly? Is there anything weird about them? This is an essential step.
3. We will be partitioning, as with RAxML. This file already has some partitions, but some overlap (coding vs the three codon positions). Delete the coding partition using the `-` sign button on the bottom of the window.
4. Partitions can be linked: allow different rates but the same topology, for example. Select all four partitions and click Link Trees. Then click on the first pull down (for noncoding) for the Tree column and name it "tree". Do the same for Clock (rename to "clock").
5. Temporarily link sites in the same way.
6. BEAST has a variety of models. We're going to do HKY (so, two different rates) with gamma-distributed rate differences. To do this:
    + Set Gamma Category Count to 4
    + Set the Shape to be estimated
    + Select the HKY model
    + Make sure Kappa is estimated
    + Make sure frequences are estimated
    + Select estimate for Substitution Rate
7. Go back to partitions and click on Unlink Site Models. This lets each partition have its own HKY+gamma model (and doing the link -> set -> unlink lets you save on work of setting it for each one).
8. We need to choose a clock model. A Strict Clock is the classic molecular clock. Most people using BEAST (and this is based on various sim studies) use relaxed clock log normal, so choose that. Having the number of discrete rates set to -1 will allow as many rates as branches.
9. Now comes the tricky bit: setting your priors. For example, you need a prior for the tree: assume a Yule prior (only speciation events, no extinction)? Or a birth-death one? [and think about the implications of these choices for later analyses -- estimating extinction rates, for example]. And even if you have a belief about whether extinction might have happened or not, what about parameters like gamma shape for first codon position sites? Exponential, beta, etc.? You probably don't have a good idea of what they should be in terms of shape, let alone priors for the actual values. And this might affect your analyses: the result is due to the prior and the likelihood. Let's leave all the priors set to the default for now, except for the tree: do a birth-death model for that.
10. We can also add priors: say, a prior for the age of a node.
    + Click on the "+" button
    + Call the Taxon set label "human-chimp"
    + Click on Homo_sapiens and then the ">>" button
    + Do the same for Pan
    + Click ok
    + Force it to be monophyletic
    + Choose Log Normal for the age prior
    + Select mean in real space (so it's easier for us to understand the age)
    + Enter 6 (for 6 MY) for the M = mean of the distribution
    + **Select a value for S that leads to a 95% range of about 5-7 MY (use the information on the right side of the window to help)**
11. Go to MCMC to set parameters for the run
    + Do 1M generations to start
    + You can also set how often the info is saved to disk or printed on the screen. Change the tracelog to primates_birthdeath_prior.log
12. Save this in the same folder as your original nexus file: maybe store as primates_birthdeath_prior.xml.
13. Yay! We have created a file with commands to run BEAST. Open it in a text editor and look at it (don't modify or save it).
14. Now open BEAST. Choose the xml file you just made and run.
15. Time passes.
16. Now open the log file in Tracer. Investigate some of the statistics, including looking at ESS: effective sample size. Ones that aren't black indicate ones that did not run long enough.
17. Go back to BEAUTi. 
    + Change the tree prior to a Yule tree
    + Change the tracelog to primates_yule_prior.log and change the tree file name
    + Save as the file to primates_yule_prior.xml
18. Use this new xml file to run BEAST again.
19. Now look at this log file with the other one, both in Tracer. Are the estimates the same? Even for something like tree height? What does this suggest?
20. Use TreeAnnotator on one of your tree files to summarize.
    + Decide what the burnin should be: that is the number of trees (not number of generations) to delete.
    + Change Posterior probability limit to 0: if you are going to show an edge, you should show the support for that. Many people only show support above 50% but still show all branches: this is problematic (only showing uncertainty on the edges where uncertainty is relatively low).
21. FigTree or R can be used to visualize the final tree with support.


##r8s
r8s implements several functions for converting a phylogram to a chronogram. It does the classic, Langley-Fitch molecular clock which stretches branches but assumes a constant rate for all. It also implements two algorithms by Sanderson: nonparametric rate smoothing (NPRS) and penalized likelihood (PL). Both relax the assumption of constant rate of evolution and instead allow rates to vary along the tree. NPRS tries to minimize rate changes at nodes. PL has a model for changes (to give likelihood of original branch lengths) and combines this with a nonparametric penalty for rate changes. It tries to minimize the combination of these two parameters, but there is a user-set penalty to decide the relative value of these in the combined sum. This is set by cross-validation: delete some data, estimate parameters, predict the deleted data, and see how close the deleted data are to the simulated data. In this case, the datum deleted is a single tip, and the length of this branch is the value to predict. This can now happen within r8s. In general, PL is more accurate than NPRS, but is slower (but both are much faster than BEAST). `treepl` is a later program that implements Sanderson's algorithms but can work on much larger trees.

Jon Eastman coded an interface to this in Geiger, but it wasn't exposed to users. I've added some additional features (including an `ez.run` mode) and documentation to his code. This is now in a fork of Geiger, but I'll file a pull request soon to put it into the main code. For now, make sure you have installed the forked version:

```
devtools::install_github("bomeara/geiger-v2")
library(geiger)
```

Then use the help for `?r8s.phylo` to figure out how to use this function. Also look at the [r8s manual](http://loco.biosci.arizona.edu/r8s/r8s1.7.manual.pdf) to understand the options.

**Run the examples** in Geiger for this. You can also look at the examples that come with r8s.

## Applying to your own work

By this point in the course, you should be thirsting to apply these tools to your own questions. Do so! **Get a dataset (think back to the getting trees method), infer a tree (if needed), and date it using one of these approaches.** I'd advise making your own github repository for this. You could pay to keep it secret; I'd advise it's probably not worth it (there is some risk of being scooped, but it's pretty low) but it's your call. 



<!--chapter:end:96_Dating.Rmd-->

---
title: 'PhyloMeth: Build a method'
author: "Brian O'Meara"
output: html_document
date: '`r format(Sys.time(), "Last updated %H:%M:%S %p %d %B, %Y")`'
---



##Objectives
By the end of these two weeks, you will:

* Identify a problem
* Figure out ways to address it
* In the final week, you will test it.

Why this? You've been exposed to a lot of approaches in phylogenetics to make a tree, date it, figure out support, and then use it to answer questions. But this is a tiny subset of all the available methods. Moreover, the methods we have today are just a subset of what we can have in the future. 

More importantly, I find students can tend to be drawn to hot methods. If you're a taxon-focused person, you might say, "I really want to study pinniped evolution, so let me find what questions they're relevant for" and then choose diversification, trait evolution, etc. and an available method. But a different approach is to start from the question, find the taxon to use to answer it, and then find the best methods to use to go after that. That's what I want you to do for the next couple weeks, with two caveats: 1) it's ok to keep the taxon focus on your study organisms, 2) do a question you can't quite answer with existing tools. Once you have this, we'll talk in class about how to go about creating methods to answer this.

<!--chapter:end:98_BuildAMethod.Rmd-->

`r if (knitr:::is_html_output()) '# References {-}'`


<!--chapter:end:99_references.Rmd-->

# Appendix

Maybe include vignettes: see http://stackoverflow.com/questions/17593912/insert-portions-of-a-markdown-document-inside-another-markdown-document-using-kn

```{r echo=FALSE, results='asis'}


#library(RCurl)
#data <- getURL("https://raw.githubusercontent.com/thej022214/hisse/master/vignettes/hisse-vignette.Rmd",
#               ssl.verifypeer=0L, followlocation=1L)

#cat(knitr::knit_child(text = data), sep = '\n')
```

<!--chapter:end:A1_appendix.Rmd-->

